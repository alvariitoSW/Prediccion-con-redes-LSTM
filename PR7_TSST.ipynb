{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qncY3FktdgMI"
      },
      "source": [
        "# TSST - Práctica 7: Predicción con redes LSTM\n",
        "\n",
        "**Alicia Lozano Díez**\n",
        "\n",
        "Jueves 10-24 de abril de 2025 / Viernes 11-25 de abril de 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6yNO58J-7my"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "El objetivo de esta práctica es proporcionar una introducción al uso de redes neuronales recurrentes (RNN) y en particular, a las Long Short-Term Memory (LSTM) para el procesamiento de series temporales y su uso en predicción. Para ello, se dividirá en dos partes basadas en dos ejemplos de datos: predicción del número de pasajeros en una aerolínea, y el problema de detección de actividad de voz (Voice Activity Detection, VAD). Esta última incluirá además una introducción al procesamiento de señales de voz.\n",
        "\n",
        "### Materiales\n",
        "\n",
        "- Guión (.ipynb) de la práctica - Moodle\n",
        "- Ejemplos de datos y etiquetas - Moodle\n",
        "- Listas de entrenamiento y validación VAD - Moodle\n",
        "- Scripts de descarga de datos - Moodle\n",
        "- Datos y etiquetas de entrenamiento VAD * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EeHT_NXP56FLkKffjyOhfa8BqAy3EmIrMkBZ0wnyDAti1g?download=1)\n",
        "- Datos y etiquetas de validación VAD * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/ESc5XzkpZ3ZBnGFQ6HWdn_UB38NHMOLTLtTcEE_b81Cylw?download=1)\n",
        "\n",
        "\n",
        "**CUIDADO: Los datos proporcionados son de uso exclusivo para esta práctica. No tiene permiso para copiar, distribuir o utilizar el corpus para ningún otro propósito.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQTITw2AxPXP"
      },
      "source": [
        "# Parte 1: Predicción mediante LSTM del número de pasajeros de una aerolínea\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjLOkaDeWrm5"
      },
      "source": [
        "## 1.1. Carga y visualización de los datos\n",
        "\n",
        "Igual que hicimos en la práctica 5, vamos a descargar el fichero con los datos de la base de datos *AirPassengers.csv* de Moodle, que contiene los datos del número de pasajeros de una aerolínea para un período de tiempo.\n",
        "\n",
        "A continuación, los cargamos en Google Colab y los visualizamos para comprobar que es el resultado esperado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA5QaJIoxbqM"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2sosWp0xoOx"
      },
      "outputs": [],
      "source": [
        "# Carga y visualizacion de los datos\n",
        "from pandas import read_csv\n",
        "\n",
        "data = read_csv('AirPassengers.csv', header=0, parse_dates=[0], index_col=0,\n",
        "                date_format='%Y-%m')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(data)\n",
        "plt.title('Serie temporal: Air Passengers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGoZB0WDXK-m"
      },
      "source": [
        "## 1.2. Normalización o escalado\n",
        "\n",
        "Las redes neuronales en general encuentran más dificultades para tratar con datos cuyos valores no tienen algún tipo de normalización.\n",
        "\n",
        "Por ello, vamos a aplicar un escalado de los datos a valores entre 0 y 1 de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wgGKEm8xxCl"
      },
      "outputs": [],
      "source": [
        "# 2. Normalización de los datos\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_norm = scaler.fit_transform(data)\n",
        "\n",
        "plt.plot(data_norm)\n",
        "plt.title('Serie temporal: Air Passengers (normalizada)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVXy-0zoYUf1"
      },
      "source": [
        "## 1.3. Preparación de los datos y definición del modelo LSTM\n",
        "\n",
        "Para el uso de modelos LSTM, vamos a utilizar la librería Pytorch (https://pytorch.org/docs/stable/index.html).\n",
        "\n",
        "En particular, vamos a definir un modelo con un número de capas LSTM dadas, y una capa de salida _fully connected_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VTyXeBG0Tqp"
      },
      "outputs": [],
      "source": [
        "# 4. Definición del modelo LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)  # Salida de la última capa LSTM\n",
        "        hn = hn[-1]  # Seleccionamos el último estado oculto\n",
        "        return self.fc(hn)\n",
        "\n",
        "# Parámetros del modelo\n",
        "input_size = 1\n",
        "hidden_size = 20\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4xWxHClZjCu"
      },
      "source": [
        "Como se observa, los parámetros utilizados serían los siguientes:\n",
        "\n",
        "* Tamaño de la entrada a la capa LSTM: input_size\n",
        "* Unidades (celdas) de la capa LSTM: hidden_size\n",
        "* Unidades de salida: output_size (capa lineal)\n",
        "\n",
        "Además, la red LSTM espera un tensor de entrada con el tamaño del batch, la longitud de la secuencia así como la dimensionalidad del espacio de características.\n",
        "\n",
        "La función forward permite obtener la predicción de la salida para un dato (o batch) de entrada (realiza el paso forward del modelo).\n",
        "\n",
        "**PREGUNTA 1: Revise la documentación de _torch.nn.LSTM_ y preste atención a los argumentos batch_first, bidirectional y dropout. ¿Para qué sirven dichos argumentos? ¿Cuál es por tanto la dimensionalidad del tensor de entrada esperado en el modelo anterior?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-vK6UfqSqB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UynaZr0aLt9"
      },
      "source": [
        "Para preparar las secuencias para la red LSTM a partir de los datos, utilizamos el siguiente código, que además divide la serie temporal (nuestros datos) en un 80\\% para entrenamiento y un 20\\% para test (o validación)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp2thzRoyzMv"
      },
      "outputs": [],
      "source": [
        "# 3. Preparación de los datos en secuencias para LSTM\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data[i:i + seq_length]\n",
        "        label = data[i + seq_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "seq_length = 12  # Ventana de 12 meses\n",
        "data = data_norm.squeeze(-1)\n",
        "X, y = create_sequences(data, seq_length)\n",
        "\n",
        "# División de datos en entrenamiento y prueba\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Conversión a tensores\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZlyP4Tuae9r"
      },
      "source": [
        "**PREGUNTA 2: Compruebe que las dimensiones de los datos de entrada son las esperadas e indique qué es cada valor.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMpNQf27q118"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVP5D01UargQ"
      },
      "source": [
        "A continuación, definimos el resto de parámetros para realizar el entrenamiento del modelo, como son el optimizador, la función de coste y los demás hiperparámetros del entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PazyWftfbDc8"
      },
      "outputs": [],
      "source": [
        "# 5. Definición del optimizador y función de pérdida\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 6. Entrenamiento del modelo\n",
        "epochs = 1000\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brMa8MVGq-wz"
      },
      "source": [
        "**PREGUNTA 3: ¿Qué función de coste se optimiza? ¿Por qué? ¿Qué indica el valor de _batch_size_?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kednkOb4rUdN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEMil9oYbGbb"
      },
      "source": [
        "Finalmente realizamos el entrenamiento del modelo durante las épocas indicadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5jM-Asx0jtD"
      },
      "outputs": [],
      "source": [
        "# 6. Entrenamiento del modelo\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = X_train[i:i+batch_size].unsqueeze(-1)  # Añadimos dimensión extra\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3WFgqVbOls"
      },
      "source": [
        "**PREGUNTA 4: ¿Qué observa en la función de pérdida? ¿Tiene sentido? ¿Hacen falta más épocas?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yafW4oyurWNr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wViLAH52bW7e"
      },
      "source": [
        "## 1.4. Evaluación del modelo - Predicción de nuevos datos\n",
        "\n",
        "Una vez entrenado el modelo, realizamos la predicción sobre el conjunto de test (20\\% restante de la serie):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmFdUeiy2iAe"
      },
      "outputs": [],
      "source": [
        "# 7. Predicción en el conjunto de prueba\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "model.eval()\n",
        "X_test_unsqueezed = X_test.unsqueeze(-1)\n",
        "predictions = model(X_test_unsqueezed).detach().numpy()\n",
        "\n",
        "# Desescalado de las predicciones y los valores reales\n",
        "y_test_descaled = scaler.inverse_transform(y_test.unsqueeze(-1).numpy())\n",
        "predictions_descaled = scaler.inverse_transform(predictions)\n",
        "\n",
        "# 8. Evaluación del modelo\n",
        "mse = mean_squared_error(y_test_descaled, predictions_descaled)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "\n",
        "# 9. Visualización de los resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_descaled, label='Valores reales', color='green')\n",
        "plt.plot(predictions_descaled, label='Predicciones LSTM', color='red')\n",
        "plt.title(\"Predicción del número de pasajeros con LSTM\")\n",
        "plt.xlabel(\"Índice de tiempo\")\n",
        "plt.ylabel(\"Número pasajeros\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr0sb5mtcHdX"
      },
      "source": [
        "**PREGUNTA 5: Realice diferentes entrenamientos modificando los hiperparámetros y compare los resultados. ¿Tienen sentido?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfnCsgMW_Btw"
      },
      "outputs": [],
      "source": [
        "# 7. Predicción en el conjunto de entrenamiento (\"cheating\")\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "model.eval()\n",
        "X_train_unsqueezed = X_train.unsqueeze(-1)\n",
        "predictions = model(X_train_unsqueezed).detach().numpy()\n",
        "\n",
        "# Desescalado de las predicciones y los valores reales\n",
        "y_train_descaled = scaler.inverse_transform(y_train.unsqueeze(-1).numpy())\n",
        "predictions_descaled = scaler.inverse_transform(predictions)\n",
        "\n",
        "# 8. Evaluación del modelo\n",
        "mse = mean_squared_error(y_train_descaled, predictions_descaled)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "\n",
        "# 9. Visualización de los resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_train_descaled, label='Valores reales', color='green')\n",
        "plt.plot(predictions_descaled, label='Predicciones LSTM', color='red')\n",
        "plt.title(\"Predicción del número de pasajeros con LSTM\")\n",
        "plt.xlabel(\"Índice de tiempo\")\n",
        "plt.ylabel(\"Número pasajeros\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDn6bv5grj8T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuqLKlwgc4rq"
      },
      "source": [
        "## 1.5. Aplicación a otros datos: series temporales financieras (opcional)\n",
        "\n",
        "A continuación, podemos ampliar el trabajo elaborando un modelo basado en LSTM para los datos de series temporales financieras del S&P500 (usados en la práctica 5).\n",
        "\n",
        "¿Hay diferencias a la hora de diseñar la red neuronal?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8kdM8VpeDbz"
      },
      "outputs": [],
      "source": [
        "## Ejercicio opcional: SP500\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1BzhOOn-6H4"
      },
      "source": [
        "# Parte 2: Detección de actividad de voz (VAD)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B5pd9Oqfuss"
      },
      "source": [
        "## 2.1. Introducción al procesamiento de señales temporales de voz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tgJsLk6UQQG"
      },
      "source": [
        "### 2.1.1. Descarga de ficheros de ejemplo\n",
        "\n",
        "Primero vamos a descargar el audio de ejemplo de Moodle (**audio_sample.wav**) y ejecutar las siguientes  líneas de código, que nos permitirán subir el archivo a Google Colab desde el disco local:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edF0oDBFruUG"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiWqca8brveq"
      },
      "source": [
        "Una vez cargado el fichero de audio, podemos escucharlo de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UH-jL-DBPpU"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "wav_file_name = \"audio_sample.wav\"\n",
        "print(wav_file_name)\n",
        "IPython.display.Audio(wav_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAujomkYr41Q"
      },
      "source": [
        "### 2.1.2. Lectura y representación de audio en Python\n",
        "\n",
        "A continuación vamos a definir ciertas funciones para poder hacer manejo de  ficheros de audio en Python.\n",
        "\n",
        "Comenzamos definiendo una función **read_recording** que leerá un fichero de audio WAV, normalizará la amplitud y devolverá el vector de muestras _signal_ y su frecuencia de muestreo _fs_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvbxNBIGBi1O"
      },
      "outputs": [],
      "source": [
        "import scipy.io.wavfile\n",
        "\n",
        "def read_recording(wav_file_name):\n",
        "  fs, signal = scipy.io.wavfile.read(wav_file_name)\n",
        "  signal = signal/max(abs(signal)) # normalizes amplitude\n",
        "\n",
        "  return fs, signal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QESNJAasdB5"
      },
      "source": [
        "Si ejecutamos la función anterior para el fichero de ejemplo, podemos ver la forma en la que se carga dicho fichero de audio en Python. Así, podemos obtener la frecuencia de muestreo y la longitud del fichero en número de muestras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISH_GeSReo8i"
      },
      "outputs": [],
      "source": [
        "fs, signal = read_recording(wav_file_name)\n",
        "print(\"Signal variable shape: \" + str(signal.shape))\n",
        "print(\"Sample rate: \" + str(fs))\n",
        "print(\"File length: \" + str(len(signal)) + \" samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njGccLOJvoWe"
      },
      "source": [
        "**PREGUNTA 6: ¿Como obtendría la duración de la señal en segundos?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68oW8lSEWw8H"
      },
      "outputs": [],
      "source": [
        "print(\"File length: \" + str(len(signal)/fs) + \" seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt1f-HXntuLS"
      },
      "source": [
        "También podemos representar la señal y ver su forma de onda. Para ello, definimos la función **plot_signal** como sigue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOzyL0JXCG65"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_signal(signal, fs, ylabel=\"\", title=\"\"):\n",
        "  dur = len(signal)/fs\n",
        "  step = 1./fs\n",
        "  t_axis = np.arange(0., dur, step)\n",
        "\n",
        "  plt.plot(t_axis, signal)\n",
        "  plt.xlim([0, dur])\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel('Time (seconds)')\n",
        "  plt.title(title)\n",
        "  plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAW3wzOxuAB-"
      },
      "source": [
        "Y utilizando la función anterior, obtenemos su representación (amplitud frente al tiempo):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpjB716Yma3O"
      },
      "outputs": [],
      "source": [
        "plot_signal(signal, fs, \"Amplitude\", wav_file_name)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzKKzBiuTWO"
      },
      "source": [
        "### 2.1.3. Representación de etiquetas de actividad de voz\n",
        "\n",
        "En esta práctica, vamos a desarrollar un detector de actividad de voz, que determinará qué segmentos de la señal de voz son realmente voz y cuáles silencio.\n",
        "\n",
        "Por ello, vamos a ver dos ejemplos de etiquetas _ground truth_, que corresponden al fichero de audio de ejemplo.\n",
        "\n",
        "Primero, descargamos de Moodle las etiquetas de voz/silencio que están en los ficheros **audio_sample_labels_1.voz** y **audio_sample_labels_2.voz** y las cargamos en Google Colab como en el caso anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM5RhBFwCx3-"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gohRGQxPFq"
      },
      "source": [
        "Estas etiquetas están guardadas en ficheros de texto y podemos cargarlas en Python de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4g5HQm6KNQL"
      },
      "outputs": [],
      "source": [
        "labels_file_name = 'audio_sample_labels_1.voz'\n",
        "voice_labels = np.loadtxt(labels_file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOarG7qcx7BV"
      },
      "source": [
        "Con el siguiente código, podemos representar la señal de voz así como sus etiquetas en la misma figura:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "telfETsXx5GK"
      },
      "outputs": [],
      "source": [
        "plot_signal(signal, fs)\n",
        "plot_signal(voice_labels*2-1, fs, \"Amplitude\", wav_file_name)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eW_pmANycy7"
      },
      "source": [
        "Las etiquetas de voz/silencio provienen de distintos detectores de actividad de voz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl3hGkNIxmuT"
      },
      "source": [
        "**PREGUNTA 7:**\n",
        "- **¿Qué valores tienen las etiquetas? ¿Qué significan dichos valores?**\n",
        "- **¿Por qué se representa _voice_labels*2-1_?**\n",
        "- **Represente la señal de voz junto con las etiquetas para ambos casos. ¿Qué diferencias observas? ¿A qué se puede deber?**\n",
        "- **¿Qué cantidad de voz/silencio hay en cada etiquetado?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHj1_1fHzAed"
      },
      "source": [
        "### 2.1.4. Extracción de características\n",
        "\n",
        "En la mayoría de sistemas de reconocimiento de patrones, un primer paso es la extracción de características. Esto consiste, a grandes rasgos, en obtener una representación de los datos de entrada, que serán utilizados para un posterior modelado.\n",
        "\n",
        "En nuestro caso, vamos pasar de la señal en crudo _\"raw\"_ dada por las muestras (_signal_), a una secuencia de vectores de características que extraigan información a corto plazo de la misma y la representen. Esta sería la entrada a nuestro sistema de detección de voz basado en redes neuronales.\n",
        "\n",
        "Para ver algunos ejemplos, vamos a utilizar la librería _librosa_ (https://librosa.org/doc/latest/index.html).\n",
        "\n",
        "Dentro de esta librería, tenemos funciones para extraer distintos tipos de características de la señal de voz, como por ejemplo el espectrograma en escala Mel (_melspectrogram_).\n",
        "\n",
        "Estas características a corto plazo, se extraen en ventanas de unos pocos milisegundos con o sin solapamiento.\n",
        "\n",
        "Un ejemplo sería el siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a_7VvYYMFnQ"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "mel_spec = librosa.feature.melspectrogram(y=signal,sr=fs,n_mels=23,win_length=320,hop_length=160)\n",
        "\n",
        "print(mel_spec.shape)\n",
        "print(signal.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQPnf6CB1Dqh"
      },
      "source": [
        "**PREGUNTA 8:**\n",
        "- **¿Qué se obtiene de la función anterior?**\n",
        "- **¿Qué significan los valores de los parámetros _win_length_ y _hop_length_?**\n",
        "- **¿Qué dimensiones de _mel_spec_ obtienes? ¿Qué significan?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihn5PM1iuSAb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwMMOuI1icD"
      },
      "source": [
        "De esta manera, podríamos obtener una parametrización de las señales para ser utilizadas como entrada a nuestra red neuronal.\n",
        "\n",
        "Para los siguientes apartados, se proporcionan los vectores de características MFCC para una serie de audios que se utilizarán como conjunto de entrenamiento del modelo de VAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64IMtv3LPTIX"
      },
      "source": [
        "## 2.2. Detector de actividad de voz (Voice Activity Detector, VAD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWgXfSysUTqQ"
      },
      "source": [
        "\n",
        "### 2.2.1. Descarga de los datos de entrenamiento\n",
        "\n",
        "Primero vamos a descargar la lista de identificadores de los datos de entrenamiento de la práctica.\n",
        "\n",
        "Para ello, necesitaremos descargar de Moodle el fichero **training_VAD.lst**, y ejecutar las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB8wcxZxVS9g"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTthDBqzPxen"
      },
      "source": [
        "A continuación cargamos los identificadores contenidos en el fichero en una lista en Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOHSTbDiVB1y"
      },
      "outputs": [],
      "source": [
        "file_train_list = 'training_VAD.lst' # mat files containing data + labels\n",
        "f = open(file_train_list, 'r')\n",
        "train_list = f.read().splitlines()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfBy_23fP-Cr"
      },
      "source": [
        "Podemos ver algunos de ellos (los primeros 10 identificatores) de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLIzBjxNQB9I"
      },
      "outputs": [],
      "source": [
        "print(train_list[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Bexbe3QSpW"
      },
      "source": [
        "Ahora, descargaremos de Moodle el fichero **data_download_onedrive_training_VAD.sh**, y ejecutaremos las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHJX10CN3KhK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gonGnfmiQhmR"
      },
      "source": [
        "Para descargar el conjunto de datos desde One drive, ejecutamos el script cargado anteriormente de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2NmZBo-34xk"
      },
      "outputs": [],
      "source": [
        "!chmod 755 data_download_onedrive_training_VAD.sh\n",
        "!./data_download_onedrive_training_VAD.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkshERjWQx_-"
      },
      "source": [
        "Este script descargará los datos de One Drive y los cargará en Google Colab, descomprimiéndolos en la carpeta **data/training_VAD**.\n",
        "\n",
        "Podemos comprobar que los ficheros **.mat** se encuentran en el directorio esperado:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6Cr06cf4pe0"
      },
      "outputs": [],
      "source": [
        "!ls data/training_VAD/ | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTwj-3NjSpJx"
      },
      "source": [
        "### 2.2.2. Definición del modelo\n",
        "\n",
        "Utilizando nuevamente la librería Pytorch, vamos a definir un modelo inicial con una capa LSTM y una capa de salida. La capa de salida estará formada por una única neurona. La salida indicará la probabilidad de voz/silencio utilizando una función *sigmoid*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUBhgYruUKfI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model_1(nn.Module):\n",
        "    def __init__(self, feat_dim=20):\n",
        "        super(Model_1, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(feat_dim,256,batch_first=True,bidirectional=False)\n",
        "        self.output = nn.Linear(256,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.lstm(x)[0]\n",
        "        out = self.output(out)\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out.squeeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-pK445hTOvq"
      },
      "source": [
        "**PREGUNTA 9: En este modelo, estamos utilizando una única neurona a la salida. ¿Hay alguna otra alternativa? ¿Se seguiría utilizando una función _sigmoid_?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8C7TQZ7umxT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1M06GDqU31I"
      },
      "source": [
        "Una vez definida la clase, podemos crear nuestra instancia del modelo y cargarlo en la GPU con el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGPBGVSkUacV"
      },
      "outputs": [],
      "source": [
        "model = Model_1(feat_dim=20)\n",
        "model = model.to(torch.device(\"cuda\"))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2BAp7tVC1A"
      },
      "source": [
        "Nuestra variable _model_ contiene el modelo, y ya estamos listos para entrenarlo y evaluarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZFacm445T6X"
      },
      "source": [
        "### 2.2.3. Lectura y preparación de los datos para el entrenamiento\n",
        "\n",
        "Como hemos visto anteriormente, nuestros datos están guardados en ficheros de Matlab (**.mat**). Cada uno de estos ficheros contiene una matriz **X** correspondiente a las secuencias de características MFCC (con sus derivadas de primer y segundo orden), y un vector **Y** con las etiquetas de voz/silencio correspondientes.\n",
        "\n",
        "Veamos un ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwLhQiBbVnpZ"
      },
      "outputs": [],
      "source": [
        "features_file = 'data/training_VAD/features_labs_1.mat'\n",
        "\n",
        "import scipy.io\n",
        "features = scipy.io.loadmat(features_file)['X']\n",
        "labels = scipy.io.loadmat(features_file)['Y']\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BYGGYYiWNgV"
      },
      "source": [
        "Elija un fichero de entrenamiento y observe tanto el tamaño de **features** como de **labels**. Estas dimensiones se corresponden con la dimensionalidad de las características (20 coeficientes MFCC en nuestro caso) y la otra dimensión es la longitud de la secuencia en número de ventanas o frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifx3j_eou3vp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEhUjoWrXpBj"
      },
      "source": [
        "El entrenamiento del modelo se va a realizar mediante descenso por gradiente (o alguna de sus variantes) basado en _batches_.\n",
        "\n",
        "Para preparar cada uno de estos _batches_ que servirán de entrada a nuestro modelo LSTM, debemos almacenar las características en secuencias de la misma longitud. El siguiente código lee las características (**get_fea**) y sus correspondientes etiquetas (**get_lab**) de un fragmento aleatorio del fichero de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNAp_LJiUjAV"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "def get_fea(segment, rand_idx):\n",
        "    data = scipy.io.loadmat(segment)['X']\n",
        "    if data.shape[0] <= length_segments:\n",
        "        start_frame = 0\n",
        "    else:\n",
        "        start_frame = np.random.permutation(data.shape[0]-length_segments)[0]\n",
        "\n",
        "    end_frame = np.min((start_frame + length_segments,data.shape[0]))\n",
        "    rand_idx[segment] = start_frame\n",
        "    feat = data[start_frame:end_frame,:]\n",
        "    return feat[np.newaxis, :, :]\n",
        "\n",
        "\n",
        "def get_lab(segment, rand_idx):\n",
        "    data = scipy.io.loadmat(segment)['Y']\n",
        "    start_frame = rand_idx[segment]\n",
        "    end_frame = np.min((start_frame + length_segments, data.shape[0]))\n",
        "    labs = data[start_frame:end_frame].flatten()\n",
        "    return labs[np.newaxis,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wek4EaRoanBq"
      },
      "source": [
        "**PREGUNTA 10: Analice las funciones anteriores detenidamente. ¿De qué tamaño son los fragmentos que se están leyendo? ¿Para qué sirve _rand_idx_?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwApxriavRRT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrBM4q80bBEK"
      },
      "source": [
        "### 2.2.4. Entrenamiento del modelo\n",
        "Una vez definidas las funciones de lectura de datos y preparación del formato que necesitamos para la entrada a la red LSTM, podemos utilizar el siguiente código para entrenarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xDupSk1U2li"
      },
      "outputs": [],
      "source": [
        "length_segments = 300\n",
        "path_in_feat = 'data/training_VAD/'\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch_size = 51\n",
        "segment_sets = np.array_split(train_list, len(train_list)/batch_size)\n",
        "\n",
        "max_iters = 5\n",
        "for epoch in range(1, max_iters):\n",
        "  print('Epoch: ',epoch)\n",
        "  model.train()\n",
        "  cache_loss = 0\n",
        "  num_correct = 0\n",
        "  num_total = 0\n",
        "\n",
        "  for ii, segment_set in enumerate(segment_sets):\n",
        "    rand_idx = {}\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Create training batches\n",
        "    train_batch = np.vstack([get_fea(path_in_feat + segment, rand_idx) for segment in segment_set])\n",
        "    labs_batch = np.vstack([get_lab(path_in_feat + segment, rand_idx).astype(np.int16)  for segment in segment_set])\n",
        "    assert len(labs_batch) == len(train_batch) # make sure that all frames have defined label\n",
        "    # Shuffle the data and place them into Pytorch tensors\n",
        "    shuffle = np.random.permutation(len(labs_batch))\n",
        "    labs_batch = torch.tensor(labs_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
        "    train_batch = torch.tensor(train_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
        "\n",
        "    # Forward the data through the network\n",
        "    outputs = model(train_batch)\n",
        "\n",
        "    # Compute cost\n",
        "    loss = criterion(outputs, labs_batch)\n",
        "\n",
        "    # Backward step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    cache_loss += loss.item()\n",
        "\n",
        "    # Estimate the number of correct outputs (acc)\n",
        "    num_correct += (outputs.round()).eq(labs_batch).sum().item()\n",
        "    num_total += (train_batch.size()[0]*train_batch.size()[1])\n",
        "    acc = num_correct * 1.0 / num_total\n",
        "\n",
        "  print(\"Loss / accuracy: \" + str(cache_loss/len(train_batch)) + '/' + str(acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h52xtcubSnD"
      },
      "source": [
        "**PREGUNTA 11:**\n",
        "**Analizar el código anterior cuidadosamente y ejecutarlo. A continuación, responder a las siguientes cuestiones:**\n",
        "- **¿Qué función de coste se está optimizando? Describir brevemente con ayuda de la documentación.**\n",
        "- **¿Qué optimizador se ha definido?**\n",
        "- **¿Para qué se utiliza _batch_size_?**\n",
        "- **Describir brevemente la creación de los _batches_.**\n",
        "- **¿Qué línea de código realiza el _forward pass_?**\n",
        "- **¿Qué línea de código realiza el _backward pass_?**\n",
        "- **¿Cuántas iteraciones del algoritmo ha realizado? ¿Qué observa en la evolución de la función de coste?**\n",
        "- **Añada al código el cálculo de la precisión o _accuracy_, de tal manera que se muestre por pantalla dicho valor en cada iteración (similar a lo que ocurre con el valor del coste _loss_). Copiar el código en el informe y describir brevemente.**\n",
        "- **¿Qué valor de coste y _accuracy_ obtiene? ¿Cómo se puede mejorar?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oACDzYgmvyRZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdthIM0MwjJ-"
      },
      "source": [
        "### 2.2.5. Evaluación del modelo: un único fichero de test\n",
        "\n",
        "Una vez entrenado el modelo, vamos a evaluarlo en un ejemplo en concreto.\n",
        "\n",
        "Descargue de Moodle el fichero **audio_sample_test.wav**, con sus correspondientes características y etiquetas **audio_sample_test.mat** y evalúe el rendimiento en el mismo, observando por ejemplo el _accuracy_ obtenido.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w13AzmgcwKQM"
      },
      "outputs": [],
      "source": [
        "# Código de evaluación aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylhS1RiAwjJ_"
      },
      "source": [
        "\n",
        "\n",
        "A continuación, represente 10 segundos de dicho audio, así como sus etiquetas de _ground_truth_ y las obtenidas con su modelo.\n",
        "\n",
        "**PREGUNTA 12: Visualmente, ¿es bueno el modelo?**\n",
        "**También puede escuchar el audio para evaluarlo cualitativamente.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdL1w0NywdLZ"
      },
      "outputs": [],
      "source": [
        "# Representación 10 segundos + etiquetas estimadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722dWQLvQDB7"
      },
      "source": [
        "### 2.2.6. Evaluación del modelo: conjunto de validación\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBKhlbtOR1Rt"
      },
      "source": [
        "Ahora vamos a evaluar el rendimiento del modelo anterior sobre un conjunto de validación (del que conocemos sus etiquetas).\n",
        "\n",
        "Para este conjunto de datos, descargaremos la lista de identificadores **valid_VAD.lst** de Moodle, así como el fichero de descarga de datos **data_download_onedrive_valid_VAD.sh**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJs0XS6rSIOf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!chmod 755 data_download_onedrive_valid_VAD.sh\n",
        "!./data_download_onedrive_valid_VAD.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDpCsrORqqj"
      },
      "source": [
        "Escriba ahora el código necesario para evaluar el modelo anterior en el conjunto de datos de validación, para su última época.\n",
        "\n",
        "Tenga en cuenta que si quiere realizar el forward para todos los datos de validación de una vez, necesitará que todas las secuencias sean de la misma longitud. Como aproximación, puede escoger unos pocos segundos de cada fichero como se hace en el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmiIbHbKRvON"
      },
      "outputs": [],
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ4V2xRvRmcE"
      },
      "source": [
        "**PREGUNTA 13: ¿Qué rendimiento (loss y accuracy) obtiene con este modelo (_Model_1_) en entrenamiento y en validación?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMsiE_lFw1-u"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsVxachwSa0z"
      },
      "source": [
        "## 2.3. Comparación de modelos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEUWfJmKULm6"
      },
      "source": [
        "\n",
        "### 2.3.1. Redes LSTM bidireccionales\n",
        "\n",
        "En este apartado, vamos a partir del modelo inicial (_Model_1_) y modificarlo para que la capa LSTM sea bidireccional (_Model_1B_).\n",
        "\n",
        "Entrene el nuevo modelo y compare el resultado con el modelo inicial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4gAYAiUTGsB"
      },
      "outputs": [],
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Iv9U1ZTFzP"
      },
      "source": [
        "**PREGUNTA 14:**\n",
        "- **Explique brevemente la diferencia entre una capa LSTM y una BLSTM (bidirectional LSTM).**\n",
        "- **¿Qué modelo obtiene un mejor resultado sobre los datos de validación? ¿Por qué puede ocurrir esto?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "785XTRKOxOPE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCYUXhQHTTH6"
      },
      "source": [
        "### 2.3.2. Modelo \"más profundo\"\n",
        "\n",
        "En este apartado, vamos a partir nuevamente del modelo _Model_1_ y vamos a añadir una segunda capa LSTM tras la primera, con el mismo tamaño y configuración, definiendo un nuevo modelo _Model_2_.\n",
        "\n",
        "Entrénelo y compare los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG-87vDhTeHB"
      },
      "outputs": [],
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEnYNMZ1TjOa"
      },
      "source": [
        "**PREGUNTA 15:**\n",
        "- **¿Qué modelo obtiene un mejor resultado sobre los datos de validación, _Model_1_ o _Model_2_? ¿Por qué puede ocurrir esto?**\n",
        "- **Y con respecto a _Model_1B_, ¿cuál es mejor?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuYuQn3_xb--"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
